{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic: Predicting Survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses data from Kaggle competition \"Titanic: Machine Learning from Disaster\" to predict which passengers survived the tragedy. Firstly, I engineer features to extract maximum information from the limited data available. Then I compare performance of multiple classifiers, such as: logistic regression, SVM, k - nearest neighbours, Naive Bayes, Decision Tree Classifier. I also use ensemble methods such as random forest, bagging, boosting and voting classifier. I use grid search and cross validation to tune up the parameters of classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Daria\\\\kaggle\\\\Titanic'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "\n",
    "from pandas import DataFrame\n",
    "from time import time\n",
    "from operator import itemgetter\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "import os\n",
    "os.chdir('C:\\\\Users\\\\Daria\\\\kaggle\\\\Titanic') \n",
    "os.getcwd() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex  Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male   22      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female   38      1   \n",
       "2                             Heikkinen, Miss. Laina  female   26      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female   35      1   \n",
       "4                           Allen, Mr. William Henry    male   35      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# preview the data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      "PassengerId    418 non-null int64\n",
      "Pclass         418 non-null int64\n",
      "Name           418 non-null object\n",
      "Sex            418 non-null object\n",
      "Age            332 non-null float64\n",
      "SibSp          418 non-null int64\n",
      "Parch          418 non-null int64\n",
      "Ticket         418 non-null object\n",
      "Fare           417 non-null float64\n",
      "Cabin          91 non-null object\n",
      "Embarked       418 non-null object\n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 39.2+ KB\n",
      "----------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 90.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# get concise summary of the data\n",
    "test.info()\n",
    "print(\"----------------------------\")\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look at every feature one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embarked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Port of Embarkation: C = Cherbourg, France; Q = Queenstown, Ireland; S = Southampton, England."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     418\n",
       "unique      3\n",
       "top         S\n",
       "freq      270\n",
       "Name: Embarked, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Embarked'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     889\n",
       "unique      3\n",
       "top         S\n",
       "freq      644\n",
       "Name: Embarked, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Embarked'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In train data fill the two missing values with the most occurred value - \"S\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['Embarked'] = train['Embarked'].fillna('S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare average survival rate conditional on port of embarkment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C</td>\n",
       "      <td>0.553571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q</td>\n",
       "      <td>0.389610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S</td>\n",
       "      <td>0.339009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Embarked  Survived\n",
       "0        C  0.553571\n",
       "1        Q  0.389610\n",
       "2        S  0.339009"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group by embarked, and get the mean for survived passengers for each value in Embarked\n",
    "embark_mean = train[['Embarked', 'Survived']].groupby(['Embarked'],as_index=False).mean()\n",
    "embark_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People embarked at Cherbourg seem to have higher chances to survive. This may be because passengers who embarked at Cherbourg are different from passengers who embarked at two other ports, due to the fact that Cherbourg is the only port out of 3 located in continental Europe. If we observe all those differences in our data, then Embarked variable should not be significant and have no effect on survival when controlling for everything else. Let's proceed and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dummy variables for Embarked column & drop S\n",
    "Embarked_dummies_train  = pd.get_dummies(train['Embarked'])\n",
    "Embarked_dummies_train.drop(['S'], axis=1, inplace=True)\n",
    "\n",
    "Embarked_dummies_test  = pd.get_dummies(test['Embarked'])\n",
    "Embarked_dummies_test.drop(['S'], axis=1, inplace=True)\n",
    "\n",
    "train.drop(['Embarked'],axis=1,inplace=True)\n",
    "test.drop(['Embarked'],axis=1,inplace=True)\n",
    "\n",
    "train = train.join(Embarked_dummies_train)\n",
    "test  = test.join(Embarked_dummies_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Familysize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two features in the data: sibsp - number of siblings/spouses aboard, and parch - number of parents/children aboard. As we can not differentiate between siblings and spouses, or parents and children, it makes more sences to combine those two variable to create as family size variable as Familysize = sibsp + parch. \n",
    "\n",
    "Note for future: With respect to the family relation variables (sibsp and parch) some relations were ignored. Cousins, nephews/nieces, aunts/uncles, in-laws, non-maried couples were not included. It may be useful to trace family size using cabin, and include everyone in the cabit as a count for family size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['FamilySize'] = train['SibSp'] + train['Parch']\n",
    "test['FamilySize'] = test['SibSp'] + test['Parch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize family size around it's mean and to a unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daria\\AppData\\Local\\Dato\\Dato Launcher\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\Daria\\AppData\\Local\\Dato\\Dato Launcher\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Daria\\AppData\\Local\\Dato\\Dato Launcher\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\Daria\\AppData\\Local\\Dato\\Dato Launcher\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Daria\\AppData\\Local\\Dato\\Dato Launcher\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\Daria\\AppData\\Local\\Dato\\Dato Launcher\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "scalerFS = preprocessing.StandardScaler().fit(train[\"FamilySize\"])\n",
    "train[\"FamilySize\"] = scalerFS.transform(train[\"FamilySize\"]) \n",
    "test[\"FamilySize\"] = scalerFS.transform(test[\"FamilySize\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the survival rate with family size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f3d0278>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEACAYAAACuzv3DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYk5BLiIScwpyEZGYU5CLiMTc/weQgSrr1Ir47gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3fc5d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FamilySize_mean = train[['FamilySize', 'Survived']].groupby(['FamilySize'],as_index=False).mean()\n",
    "plt.plot(FamilySize_mean['FamilySize'], FamilySize_mean['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pattern is not linear - add squared family size variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pattern is not linear - add squared family size variable\n",
    "train['FamilySizeSq'] = train['FamilySize']*train['FamilySize']\n",
    "test['FamilySizeSq'] = test['FamilySize']*test['FamilySize']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Name Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name length may be correlated with income status, nobility, or origin, and therefore have an impact on survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['NameLength'] = train['Name'].apply(lambda x: len(x))  \n",
    "test['NameLength'] = test['Name'].apply(lambda x: len(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some descriptive statistics for name length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    891.000000\n",
       "mean      26.965208\n",
       "std        9.281607\n",
       "min       12.000000\n",
       "25%       20.000000\n",
       "50%       25.000000\n",
       "75%       30.000000\n",
       "max       82.000000\n",
       "Name: NameLength, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['NameLength'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f8225c0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEACAYAAABMEua6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRgaDMWNKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f2631d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NameLength_mean = train[['NameLength', 'Survived']].groupby(['NameLength'],as_index=False).mean()\n",
    "plt.plot(NameLength_mean['NameLength'], NameLength_mean['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that probability of survival is increasing with name size, though variance is increasing too (due to small number of observations, only 25% of observations fall above 30)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize name length around it's mean and to a unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daria\\AppData\\Local\\Dato\\Dato Launcher\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\Daria\\AppData\\Local\\Dato\\Dato Launcher\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Daria\\AppData\\Local\\Dato\\Dato Launcher\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\Daria\\AppData\\Local\\Dato\\Dato Launcher\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Daria\\AppData\\Local\\Dato\\Dato Launcher\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\Daria\\AppData\\Local\\Dato\\Dato Launcher\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "scalerNL = preprocessing.StandardScaler().fit(train['NameLength'])\n",
    "train['NameLength'] = scalerNL.transform(train['NameLength']) \n",
    "test['NameLength'] = scalerNL.transform(test['NameLength']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract title from a name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_title(name):\n",
    "    # Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr          517\n",
       "Miss        182\n",
       "Mrs         125\n",
       "Master       40\n",
       "Dr            7\n",
       "Rev           6\n",
       "Col           2\n",
       "Major         2\n",
       "Mlle          2\n",
       "Countess      1\n",
       "Ms            1\n",
       "Lady          1\n",
       "Jonkheer      1\n",
       "Don           1\n",
       "Mme           1\n",
       "Capt          1\n",
       "Sir           1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all the titles\n",
    "titles_train = train['Name'].apply(get_title)\n",
    "titles_test = test['Name'].apply(get_title)\n",
    "\n",
    "# print counts for each title in training set\n",
    "pd.value_counts(titles_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_mapping = {\"Mr\": 1,\n",
    "                 \"Miss\": 2, \"Mlle\": 2, \"Mme\": 2,\n",
    "                 \"Mrs\": 3, \"Ms\": 3, \"Dona\": 3, \"Lady\": 3, \"Countess\": 3,\n",
    "                 \"Master\": 4,\n",
    "                 \"Dr\": 5,\n",
    "                 \"Rev\": 6,\n",
    "                 \"Major\": 7, \"Col\": 7, \"Sir\": 7, \"Don\": 7, \"Jonkheer\": 7, \"Capt\": 7,\n",
    "                 }\n",
    "for k,v in title_mapping.items():\n",
    "    titles_train[titles_train == k] = v\n",
    "    titles_test[titles_test == k] = v\n",
    "\n",
    "# Add in the title column.\n",
    "train['Title'] = titles_train\n",
    "test['Title'] = titles_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.156673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.702703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.796875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Title  Survived\n",
       "5      6  0.000000\n",
       "0      1  0.156673\n",
       "6      7  0.375000\n",
       "4      5  0.428571\n",
       "3      4  0.575000\n",
       "1      2  0.702703\n",
       "2      3  0.796875"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean survival for each title\n",
    "Title_mean = train[['Title', 'Survived']].groupby(['Title'],as_index=False).mean()\n",
    "Title_mean.sort('Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like all title categories had distinct mean survival. Grown up females on average survived more often, then young females and young males. None of The Reverends in the sample survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dummy variables for Title column & drop 1 (Mr)\n",
    "Title_dummies_train  = pd.get_dummies(train['Title'], prefix = 'Title')\n",
    "Title_dummies_train.drop(['Title_1'], axis=1, inplace=True)\n",
    "\n",
    "Title_dummies_test  = pd.get_dummies(test['Title'], prefix = 'Title')\n",
    "Title_dummies_test.drop(['Title_1'], axis=1, inplace=True)\n",
    "\n",
    "train = train.join(Title_dummies_train)\n",
    "test  = test.join(Title_dummies_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fare has one missing value in test set - replace with median fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['Fare'] = test['Fare'].fillna(test['Fare'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f697fd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA30AAADICAYAAABcZYjiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f2469e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "train['Fare'].plot(kind='hist', figsize=(15,3),bins=100, xlim=(0,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fare is right skewed. Log transformartion will improve skew and kurtosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['Fare'] = np.log(train['Fare']+1)\n",
    "test['Fare'] = np.log(test['Fare']+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And normalize around zero with unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daria\\AppData\\Local\\Dato\\Dato Launcher\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Daria\\AppData\\Local\\Dato\\Dato Launcher\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Daria\\AppData\\Local\\Dato\\Dato Launcher\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "scalerF = preprocessing.StandardScaler().fit(train['Fare'])\n",
    "train['Fare'] = scalerF.transform(train['Fare']) \n",
    "test['Fare'] = scalerF.transform(test['Fare']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passenger Class: 1 = 1st, 2 = 2nd, 3 = 3rd. It is a proxy for socio-economic status (SES) 1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.629630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.472826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.242363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Survived\n",
       "0       1  0.629630\n",
       "1       2  0.472826\n",
       "2       3  0.242363"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pclass_mean = train[['Pclass', 'Survived']].groupby(['Pclass'],as_index=False).mean()\n",
    "Pclass_mean.sort('Pclass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher class passengers have higher survival rate, but the ralition is not linear, thus we need to convert Pclass into dummy variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers\n",
    "pclass_dummies_titanic  = pd.get_dummies(train['Pclass'], prefix = 'Class')\n",
    "pclass_dummies_titanic.drop(['Class_3'], axis=1, inplace=True)\n",
    "\n",
    "pclass_dummies_test  = pd.get_dummies(test['Pclass'], prefix = 'Class')\n",
    "pclass_dummies_test.drop(['Class_3'], axis=1, inplace=True)\n",
    "\n",
    "train.drop(['Pclass'],axis=1,inplace=True)\n",
    "test.drop(['Pclass'],axis=1,inplace=True)\n",
    "\n",
    "train = train.join(pclass_dummies_titanic)\n",
    "test  = test.join(pclass_dummies_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    714.000000\n",
       "mean      29.699118\n",
       "std       14.526497\n",
       "min        0.420000\n",
       "25%       20.125000\n",
       "50%       28.000000\n",
       "75%       38.000000\n",
       "max       80.000000\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    332.000000\n",
       "mean      30.272590\n",
       "std       14.181209\n",
       "min        0.170000\n",
       "25%       21.000000\n",
       "50%       27.000000\n",
       "75%       39.000000\n",
       "max       76.000000\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Age'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20% of Age data is missing for both training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['AgeNull'] = train['Age'].isnull()\n",
    "test['AgeNull'] = test['Age'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AgeNull</th>\n",
       "      <th>Survived</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.406162</td>\n",
       "      <td>0.512605</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.082361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>0.293785</td>\n",
       "      <td>0.564972</td>\n",
       "      <td>0.180791</td>\n",
       "      <td>-0.332235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AgeNull  Survived     SibSp     Parch      Fare\n",
       "0   False  0.406162  0.512605  0.431373  0.082361\n",
       "1    True  0.293785  0.564972  0.180791 -0.332235"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AgeNull_means = train[['AgeNull', 'Survived']].groupby(['AgeNull'],as_index=False).mean()\n",
    "AgeNull_SibSp = train[['AgeNull', 'SibSp']].groupby(['AgeNull'],as_index=False).mean()\n",
    "AgeNull_means['SibSp'] = AgeNull_SibSp['SibSp']\n",
    "AgeNull_Parch = train[['AgeNull', 'Parch']].groupby(['AgeNull'],as_index=False).mean()\n",
    "AgeNull_means['Parch'] = AgeNull_Parch['Parch']\n",
    "AgeNull_Fare = train[['AgeNull', 'Fare']].groupby(['AgeNull'],as_index=False).mean()\n",
    "AgeNull_means['Fare'] = AgeNull_Fare['Fare']\n",
    "AgeNull_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average survival rate, fare and number of parents/childred is different for individuals with missing age data. Therefore, I suspect that the data is not missing at random. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use both train and test data to model missing values in age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_wo = train.drop(['Survived'], axis=1)\n",
    "all_data = pd.concat((train_wo, test), axis=0, ignore_index=True) # concatenate train and test data into one table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing = all_data.index[all_data['Age'].isnull()]\n",
    "non_missing = all_data.index[all_data['Age'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use linear regression to predict missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daria\\AppData\\Local\\Dato\\Dato Launcher\\lib\\site-packages\\pandas\\core\\indexing.py:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "alg = LinearRegression()\n",
    "\n",
    "predictors = ['Title_2','Title_3','Title_4','Title_5','Title_6','Title_7', \n",
    "              'SibSp', 'Parch', 'Fare', 'Class_1','Class_2', 'C', 'Q']\n",
    "\n",
    "train_predictors = (all_data[predictors].iloc[non_missing,:])\n",
    "# The target we're using to train the algorithm.\n",
    "train_target = all_data['Age'].iloc[non_missing]\n",
    "# Training the algorithm using the predictors and target.\n",
    "alg.fit(train_predictors, train_target)\n",
    "# We can now make predictions on the missing data\n",
    "all_data['Age'].iloc[missing] =  alg.predict(all_data[predictors].iloc[missing,:])\n",
    "# Truncate values of Age at zero to be non-negative\n",
    "all_data.loc[all_data['Age']<0,'Age'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1309.000000\n",
       "mean       29.754081\n",
       "std        13.454828\n",
       "min         0.000000\n",
       "25%        22.000000\n",
       "50%        29.000000\n",
       "75%        37.000000\n",
       "max        80.000000\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['Age'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting back to train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train=all_data[0:891]\n",
    "df_train = pd.concat((df_train, train[['Survived']]), axis=1) # add Survived column\n",
    "df_test=all_data[891:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not normalize age just yet, because I will use it next to define a child, and will normalize it after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw from Title, children on aboard seem to have a high chances for Survival. So, we can classify passengers as males, females, and child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daria\\AppData\\Local\\Dato\\Dato Launcher\\lib\\site-packages\\IPython\\kernel\\__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Daria\\AppData\\Local\\Dato\\Dato Launcher\\lib\\site-packages\\IPython\\kernel\\__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "def get_person(passenger):\n",
    "    age,sex = passenger\n",
    "    return 'child' if age < 16 else sex\n",
    "    \n",
    "df_train['Person'] = df_train[['Age','Sex']].apply(get_person,axis=1)\n",
    "df_test['Person']  = df_test [['Age','Sex']].apply(get_person,axis=1)\n",
    "\n",
    "# No need to use Sex column since we created Person column\n",
    "df_train.drop(['Sex'],axis=1,inplace=True)\n",
    "df_test.drop(['Sex'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>child</td>\n",
       "      <td>0.547368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>0.775665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>0.161351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Person  Survived\n",
       "0   child  0.547368\n",
       "1  female  0.775665\n",
       "2    male  0.161351"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pclass_mean = df_train[['Person', 'Survived']].groupby(['Person'],as_index=False).mean()\n",
    "Pclass_mean.sort('Person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dummy variables for Person column, & drop Male\n",
    "person_dummies_train  = pd.get_dummies(df_train['Person'])\n",
    "person_dummies_train.drop(['male'], axis=1, inplace=True)\n",
    "\n",
    "person_dummies_test  = pd.get_dummies(df_test['Person'])\n",
    "person_dummies_test.drop(['male'], axis=1, inplace=True)\n",
    "\n",
    "df_train = df_train.join(person_dummies_train)\n",
    "df_test  = df_test.join(person_dummies_test)\n",
    "\n",
    "df_train.drop(['Person'],axis=1,inplace=True)\n",
    "df_test.drop(['Person'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, normalize age around zero with unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daria\\AppData\\Local\\Dato\\Dato Launcher\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Daria\\AppData\\Local\\Dato\\Dato Launcher\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Daria\\AppData\\Local\\Dato\\Dato Launcher\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "scalerAge = preprocessing.StandardScaler().fit(df_train['Age'])\n",
    "df_train['Age'] = scalerAge.transform(df_train['Age']) \n",
    "df_test['Age'] = scalerAge.transform(df_test['Age']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop all the useless colums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.drop(['Name','SibSp', 'Parch', 'Ticket', 'Cabin', 'Title', 'AgeNull'],axis=1,inplace=True)\n",
    "df_test.drop(['Name','SibSp', 'Parch', 'Ticket', 'Cabin', 'Title', 'AgeNull'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>FamilySizeSq</th>\n",
       "      <th>NameLength</th>\n",
       "      <th>Title_2</th>\n",
       "      <th>Title_3</th>\n",
       "      <th>Title_4</th>\n",
       "      <th>Title_5</th>\n",
       "      <th>Title_6</th>\n",
       "      <th>Title_7</th>\n",
       "      <th>Class_1</th>\n",
       "      <th>Class_2</th>\n",
       "      <th>Survived</th>\n",
       "      <th>child</th>\n",
       "      <th>female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.563914</td>\n",
       "      <td>-0.879741</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.059160</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>-0.427451</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.615541</td>\n",
       "      <td>1.361220</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.059160</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>2.590962</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.269050</td>\n",
       "      <td>-0.798540</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.560975</td>\n",
       "      <td>0.314693</td>\n",
       "      <td>-0.535252</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.394394</td>\n",
       "      <td>1.062038</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.059160</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.836359</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.394394</td>\n",
       "      <td>-0.784179</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.560975</td>\n",
       "      <td>0.314693</td>\n",
       "      <td>-0.319651</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.371059</td>\n",
       "      <td>-0.738616</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.560975</td>\n",
       "      <td>0.314693</td>\n",
       "      <td>-1.182055</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1.794997</td>\n",
       "      <td>1.038146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.560975</td>\n",
       "      <td>0.314693</td>\n",
       "      <td>-0.427451</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>-2.038234</td>\n",
       "      <td>0.136499</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.919564</td>\n",
       "      <td>3.684726</td>\n",
       "      <td>0.327152</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>-0.195334</td>\n",
       "      <td>-0.481456</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.679295</td>\n",
       "      <td>0.461441</td>\n",
       "      <td>2.375361</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>-1.153642</td>\n",
       "      <td>0.489438</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.059160</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.866154</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId       Age      Fare  C  Q  FamilySize  FamilySizeSq  \\\n",
       "0            1 -0.563914 -0.879741  0  0    0.059160      0.003500   \n",
       "1            2  0.615541  1.361220  1  0    0.059160      0.003500   \n",
       "2            3 -0.269050 -0.798540  0  0   -0.560975      0.314693   \n",
       "3            4  0.394394  1.062038  0  0    0.059160      0.003500   \n",
       "4            5  0.394394 -0.784179  0  0   -0.560975      0.314693   \n",
       "5            6  0.371059 -0.738616  0  1   -0.560975      0.314693   \n",
       "6            7  1.794997  1.038146  0  0   -0.560975      0.314693   \n",
       "7            8 -2.038234  0.136499  0  0    1.919564      3.684726   \n",
       "8            9 -0.195334 -0.481456  0  0    0.679295      0.461441   \n",
       "9           10 -1.153642  0.489438  1  0    0.059160      0.003500   \n",
       "\n",
       "   NameLength  Title_2  Title_3  Title_4  Title_5  Title_6  Title_7  Class_1  \\\n",
       "0   -0.427451        0        0        0        0        0        0        0   \n",
       "1    2.590962        0        1        0        0        0        0        1   \n",
       "2   -0.535252        1        0        0        0        0        0        0   \n",
       "3    1.836359        0        1        0        0        0        0        1   \n",
       "4   -0.319651        0        0        0        0        0        0        0   \n",
       "5   -1.182055        0        0        0        0        0        0        0   \n",
       "6   -0.427451        0        0        0        0        0        0        1   \n",
       "7    0.327152        0        0        1        0        0        0        0   \n",
       "8    2.375361        0        1        0        0        0        0        0   \n",
       "9    0.866154        0        1        0        0        0        0        0   \n",
       "\n",
       "   Class_2  Survived  child  female  \n",
       "0        0         0      0       0  \n",
       "1        0         1      0       1  \n",
       "2        0         1      0       1  \n",
       "3        0         1      0       1  \n",
       "4        0         0      0       0  \n",
       "5        0         0      0       0  \n",
       "6        0         0      0       0  \n",
       "7        0         0      1       0  \n",
       "8        0         1      0       1  \n",
       "9        1         1      1       0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_predictors = ['Age', 'Fare', 'C', 'Q', 'FamilySize', 'FamilySizeSq', 'NameLength', 'Class_1','Class_2', 'child', 'female', \n",
    "                 'Title_2','Title_3','Title_4','Title_5','Title_6','Title_7']\n",
    "no_title_predictors = ['Age', 'Fare', 'C', 'Q', 'FamilySize', 'FamilySizeSq', 'NameLength', 'Class_1','Class_2', 'child', \n",
    "                       'female']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate data into training and testing data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.826815642458\n"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression()\n",
    "logistic.fit(train_data[all_predictors], train_data['Survived'])\n",
    "print logistic.score(test_data[all_predictors], test_data[\"Survived\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.23872263,  0.41594464,  0.36901852,  0.2340289 , -0.38793008,\n",
       "        -0.19824214,  0.03418313,  1.10737438,  0.76611778,  1.37045838,\n",
       "         1.6337104 ,  0.92067827,  1.70200435,  1.3129437 ,  0.18568668,\n",
       "        -0.57103816,  0.33652938]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.826815642458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.21407536,  0.39556066,  0.39032684,  0.19624627, -0.24895857,\n",
       "        -0.22406   ,  0.16239932,  1.06701097,  0.77354106,  2.17371838,\n",
       "         2.67230882]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = LogisticRegression()\n",
    "logistic.fit(train_data[no_title_predictors], train_data['Survived'])\n",
    "print logistic.score(test_data[no_title_predictors], test_data[\"Survived\"])\n",
    "logistic.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Classifier (L2 penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use k-fold cross validation to find the optimal L2 penalty for ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeClassifierCV(alphas=array([  1.     ,   1.00231, ...,   9.97698,  10.     ]),\n",
       "         class_weight=None, cv=None, fit_intercept=True, normalize=False,\n",
       "         scoring=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = RidgeClassifierCV(alphas = np.logspace(0, 1, num=1000))\n",
    "ridge.fit(train_data[all_predictors], train_data['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.1726573872160193"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82122905027932958"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.score(test_data[all_predictors], test_data['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08350369,  0.11684021,  0.14159756,  0.08250713, -0.19644965,\n",
       "        -0.00562247, -0.00510736,  0.39445519,  0.29111215,  0.48113421,\n",
       "         0.61841584,  0.37301458,  0.66558689,  0.44013222,  0.08328526,\n",
       "        -0.20636993,  0.12272852]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like L2 penaty does not improve performance of logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would not expct NB conditional independence assumption to holds in this problem (Title, Age, Female and Child are probably correlated), but still let's run the classifier and see how it performs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44692737430167595"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian = GaussianNB()\n",
    "gaussian.fit(train_data[all_predictors], train_data['Survived'])\n",
    "gaussian.score(test_data[all_predictors], test_data['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use all predictors but title and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78770949720670391"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian = GaussianNB()\n",
    "gaussian.fit(train_data[no_title_predictors], train_data['Survived'])\n",
    "gaussian.score(test_data[no_title_predictors], test_data['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the performance is much better, but a simple logistic regression still outperforms it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K -  Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try non parametric classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use grid search to tune up the parameters of KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'auto',\n",
       " 'leaf_size': 30,\n",
       " 'metric': 'minkowski',\n",
       " 'metric_params': None,\n",
       " 'n_jobs': 1,\n",
       " 'n_neighbors': 5,\n",
       " 'p': 2,\n",
       " 'weights': 'uniform'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 311.32 seconds for 240 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.829 (std: 0.189)\n",
      "Parameters: {'n_neighbors': 16, 'metric': 'manhattan', 'weights': 'uniform'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.827 (std: 0.182)\n",
      "Parameters: {'n_neighbors': 18, 'metric': 'manhattan', 'weights': 'uniform'}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.826 (std: 0.192)\n",
      "Parameters: {'n_neighbors': 17, 'metric': 'manhattan', 'weights': 'uniform'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build a classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# grid over parameters\n",
    "param_grid = {\"metric\": [\"minkowski\", \"manhattan\", \"chebyshev\"], \n",
    "              \"n_neighbors\": list(range(1,41)),\n",
    "              \"weights\": [\"uniform\", \"distance\"]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(knn, param_grid=param_grid, cv = 178)\n",
    "start = time()\n",
    "grid_search.fit(train_data[all_predictors], train_data['Survived'])\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.grid_scores_)))\n",
    "report(grid_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what will be the score on test set for the knn classifier with tuned parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81005586592178769"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 16, metric = 'manhattan', weights = 'uniform')\n",
    "knn.fit(train_data[all_predictors], train_data['Survived'])\n",
    "knn.score(test_data[all_predictors], test_data['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 440.50 seconds for 24 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.827 (std: 0.022)\n",
      "Parameters: {'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.823 (std: 0.024)\n",
      "Parameters: {'kernel': 'linear', 'C': 10, 'gamma': 0.001}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.823 (std: 0.024)\n",
      "Parameters: {'kernel': 'linear', 'C': 10, 'gamma': 0.0001}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build a classifier\n",
    "svc = SVC()\n",
    "\n",
    "# grid over parameters\n",
    "param_grid = {\"kernel\": [\"linear\", \"rbf\", \"sigmoid\"],\n",
    "              \"gamma\": [1e-3, 1e-4],\n",
    "              \"C\": [1, 10, 100, 1000]}\n",
    "                    \n",
    "# run grid search\n",
    "grid_search = GridSearchCV(svc, param_grid=param_grid, cv = 5)\n",
    "start = time()\n",
    "grid_search.fit(train_data[all_predictors], train_data['Survived'])\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.grid_scores_)))\n",
    "report(grid_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81564245810055869"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(kernel = 'rbf', C = 1000, gamma = 0.001)\n",
    "svc.fit(train_data[all_predictors], train_data['Survived'])\n",
    "svc.score(test_data[all_predictors], test_data['Survived']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible that our features are not linear, moreover, there are a lot of categorical variables in the data. Therefore, Decision Trees may perform better on our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 217.97 seconds for 6800 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.824 (std: 0.018)\n",
      "Parameters: {'max_features': 13, 'min_samples_split': 10, 'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 7}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.824 (std: 0.021)\n",
      "Parameters: {'max_features': 15, 'min_samples_split': 4, 'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 6}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.824 (std: 0.021)\n",
      "Parameters: {'max_features': 15, 'min_samples_split': 4, 'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 7}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build a classifier\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# grid over parameters\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": list(range(1,18)),\n",
    "              \"min_samples_split\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "              \"min_samples_leaf\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "                    \n",
    "# run grid search\n",
    "grid_search = GridSearchCV(tree, param_grid=param_grid, cv = 5)\n",
    "start = time()\n",
    "grid_search.fit(train_data[all_predictors], train_data['Survived'])\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.grid_scores_)))\n",
    "report(grid_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74860335195530725"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_features = 13, min_samples_split = 10, criterion = 'entropy', max_depth = 3,\n",
    "                              min_samples_leaf = 7)\n",
    "tree.fit(train_data[all_predictors], train_data['Survived'])\n",
    "tree.score(test_data[all_predictors], test_data['Survived']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07851363,  0.00417512,  0.        ,  0.        ,  0.03865113,\n",
       "        0.        ,  0.        ,  0.09888482,  0.05748556,  0.        ,\n",
       "        0.        ,  0.21626396,  0.50602578,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features with non-zero importance: 'Age', 'Fare', 'FamilySize', 'Class_1','Class_2', 'Title_2' (\"Miss\", \"Mlle\", \"Mme\"), \n",
    "'Title_3' (\"Mrs\", \"Ms\", \"Dona\", \"Lady\", \"Countess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 2183.55 seconds for 1792 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.834 (std: 0.002)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 4, 'min_samples_split': 9, 'criterion': 'entropy', 'max_features': 2, 'max_depth': None}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.834 (std: 0.014)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 7, 'min_samples_split': 5, 'criterion': 'entropy', 'max_features': 9, 'max_depth': None}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.834 (std: 0.006)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 7, 'min_samples_split': 6, 'criterion': 'entropy', 'max_features': 11, 'max_depth': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build a classifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# use a full grid over all parameters\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": list(range(2,16)),\n",
    "              \"min_samples_split\": list(range(2,10)),\n",
    "              \"min_samples_leaf\": list(range(2,10)),\n",
    "              \"bootstrap\": [False],\n",
    "              \"criterion\": [\"entropy\"]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid)\n",
    "start = time()\n",
    "grid_search.fit(train_data[all_predictors], train_data['Survived'])\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.grid_scores_)))\n",
    "report(grid_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84357541899441346"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(random_state=1, n_estimators=100, bootstrap = False, min_samples_leaf = 4,\n",
    "                                       min_samples_split = 9, max_features = 2, max_depth = None, criterion = 'entropy')\n",
    "random_forest.fit(train_data[all_predictors], train_data['Survived'])\n",
    "random_forest.score(test_data[all_predictors], test_data['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.02405099e-01,   1.54832518e-01,   1.80067603e-02,\n",
       "         7.01339593e-03,   5.40422219e-02,   4.40290527e-02,\n",
       "         1.15805081e-01,   4.41134861e-02,   3.34694703e-02,\n",
       "         1.67755287e-02,   2.12011208e-01,   9.03998503e-02,\n",
       "         9.52767297e-02,   8.97989475e-03,   1.87712747e-04,\n",
       "         2.50565916e-03,   1.46330924e-04])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 325.07 seconds for 384 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.836 (std: 0.018)\n",
      "Parameters: {'loss': 'exponential', 'learning_rate': 0.1, 'min_samples_leaf': 14, 'min_samples_split': 11, 'max_features': 2, 'max_depth': None}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.834 (std: 0.010)\n",
      "Parameters: {'loss': 'exponential', 'learning_rate': 0.1, 'min_samples_leaf': 14, 'min_samples_split': 5, 'max_features': 2, 'max_depth': None}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.833 (std: 0.012)\n",
      "Parameters: {'loss': 'deviance', 'learning_rate': 0.1, 'min_samples_leaf': 14, 'min_samples_split': 20, 'max_features': 8, 'max_depth': 3}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GB = GradientBoostingClassifier(n_estimators=100)\n",
    "\n",
    "param_grid = {\"loss\" : [\"deviance\", \"exponential\"],\n",
    "             \"learning_rate\": [0.1],\n",
    "             \"max_depth\": [3, None],\n",
    "             \"min_samples_split\": [3, 5, 8, 11, 14, 20],\n",
    "             \"min_samples_leaf\": [8, 11, 14, 17, 20],\n",
    "             \"max_features\": [2, 5, 8, 11, 14]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(GB, param_grid=param_grid)\n",
    "start = time()\n",
    "grid_search.fit(train_data[all_predictors], train_data['Survived'])\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.grid_scores_)))\n",
    "report(grid_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84357541899441346"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GB = GradientBoostingClassifier(n_estimators=100, loss = 'exponential', learning_rate = 0.1,\n",
    "                                min_samples_leaf = 14, min_samples_split = 11, max_features = 2, max_depth = None)\n",
    "GB.fit(train_data[all_predictors], train_data['Survived'])\n",
    "GB.score(test_data[all_predictors], test_data['Survived']) # Mean validation score: 0.843 (std: 0.016) # 0.75598"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 31.55 seconds for 20 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.817 (std: 0.007)\n",
      "Parameters: {'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3,\n",
      "            max_features=13, max_leaf_nodes=None, min_samples_leaf=7,\n",
      "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), 'learning_rate': 0.54555947811685146}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.815 (std: 0.006)\n",
      "Parameters: {'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3,\n",
      "            max_features=13, max_leaf_nodes=None, min_samples_leaf=7,\n",
      "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), 'learning_rate': 0.16237767391887209}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.815 (std: 0.010)\n",
      "Parameters: {'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3,\n",
      "            max_features=13, max_leaf_nodes=None, min_samples_leaf=7,\n",
      "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), 'learning_rate': 0.29763514416313192}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Ada = AdaBoostClassifier(n_estimators=100, algorithm='SAMME')\n",
    "\n",
    "#param_grid = {\"base_estimator\" : [DecisionTreeClassifier(), RidgeClassifierCV()],\n",
    "#             \"learning_rate\": [0.01, 0.5, 0.1]}\n",
    "\n",
    "param_grid = {\"base_estimator\" : [DecisionTreeClassifier(max_features = 13, min_samples_split = 10, criterion = 'entropy', \n",
    "                                                         max_depth = 3, min_samples_leaf = 7)], \n",
    "              \"learning_rate\": np.logspace(-5, 0, num=20)}\n",
    "             \n",
    "# run grid search\n",
    "grid_search = GridSearchCV(Ada, param_grid=param_grid)\n",
    "start = time()\n",
    "grid_search.fit(train_data[all_predictors], train_data['Survived'])\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.grid_scores_)))\n",
    "report(grid_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ada = AdaBoostClassifier(n_estimators=100, algorithm='SAMME', \n",
    "                         base_estimator = DecisionTreeClassifier(max_features = 13, min_samples_split = 10, criterion = 'entropy', \n",
    "                                                         max_depth = 3, min_samples_leaf = 7), learning_rate = 0.54555947811685146)\n",
    "Ada.fit(train_data[all_predictors], train_data['Survived'])\n",
    "Ada.score(test_data[all_predictors], test_data['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 531.54 seconds for 20 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.829 (std: 0.010)\n",
      "Parameters: {'base_estimator': RidgeClassifierCV(alphas=(0.1, 1.0, 10.0), class_weight=None, cv=None,\n",
      "         fit_intercept=True, normalize=False, scoring=None), 'learning_rate': 1.0000000000000001e-05}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.829 (std: 0.010)\n",
      "Parameters: {'base_estimator': RidgeClassifierCV(alphas=(0.1, 1.0, 10.0), class_weight=None, cv=None,\n",
      "         fit_intercept=True, normalize=False, scoring=None), 'learning_rate': 1.8329807108324375e-05}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.829 (std: 0.010)\n",
      "Parameters: {'base_estimator': RidgeClassifierCV(alphas=(0.1, 1.0, 10.0), class_weight=None, cv=None,\n",
      "         fit_intercept=True, normalize=False, scoring=None), 'learning_rate': 3.3598182862837813e-05}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Ada = AdaBoostClassifier(n_estimators=100, algorithm='SAMME')\n",
    "\n",
    "#param_grid = {\"base_estimator\" : [DecisionTreeClassifier(), RidgeClassifierCV()],\n",
    "#             \"learning_rate\": [0.01, 0.5, 0.1]}\n",
    "\n",
    "param_grid = {\"base_estimator\" : [RidgeClassifierCV()], \n",
    "              \"learning_rate\": np.logspace(-5, 0, num=20)}\n",
    "             \n",
    "# run grid search\n",
    "grid_search = GridSearchCV(Ada, param_grid=param_grid)\n",
    "start = time()\n",
    "grid_search.fit(train_data[all_predictors], train_data['Survived'])\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.grid_scores_)))\n",
    "report(grid_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81005586592178769"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ada = AdaBoostClassifier(n_estimators=100, algorithm='SAMME', \n",
    "                         base_estimator = RidgeClassifierCV(), learning_rate = 1.0000000000000001e-05)\n",
    "Ada.fit(train_data[all_predictors], train_data['Survived'])\n",
    "Ada.score(test_data[all_predictors], test_data['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduces variance of base estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 368.37 seconds for 52 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.827 (std: 0.022)\n",
      "Parameters: {'max_features': 8, 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.827 (std: 0.012)\n",
      "Parameters: {'max_features': 8, 'base_estimator': SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.826 (std: 0.014)\n",
      "Parameters: {'max_features': 10, 'base_estimator': SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bagging = BaggingClassifier(n_estimators=100)\n",
    "\n",
    "\n",
    "param_grid = {\"base_estimator\" : [DecisionTreeClassifier(), \n",
    "                                  RidgeClassifierCV(), \n",
    "                                  SVC(kernel = 'rbf', C = 1000, gamma = 0.001),\n",
    "                                  KNeighborsClassifier(n_neighbors = 16, metric = 'manhattan', weights = 'uniform')],\n",
    "             \"max_features\": list(range(2,15))}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(Bagging, param_grid=param_grid)\n",
    "start = time()\n",
    "grid_search.fit(train_data[all_predictors], train_data['Survived'])\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.grid_scores_)))\n",
    "report(grid_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84357541899441346"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bagging = BaggingClassifier(n_estimators=100, base_estimator = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            presort=False, random_state=None, splitter='best'), max_features = 8)\n",
    "Bagging.fit(train_data[all_predictors], train_data['Survived'])\n",
    "Bagging.score(test_data[all_predictors], test_data['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the best classifiers from above and tune up weighs. Searching over all possible weights is very time consuming, therefore, let's just search for weights in range 1 to 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>w3</th>\n",
       "      <th>w4</th>\n",
       "      <th>w5</th>\n",
       "      <th>w7</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.841335</td>\n",
       "      <td>0.022678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.841315</td>\n",
       "      <td>0.022767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.841315</td>\n",
       "      <td>0.022334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.841315</td>\n",
       "      <td>0.022334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.841295</td>\n",
       "      <td>0.020139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.839936</td>\n",
       "      <td>0.022983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.839936</td>\n",
       "      <td>0.022983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.839936</td>\n",
       "      <td>0.022983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.839936</td>\n",
       "      <td>0.021213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.839917</td>\n",
       "      <td>0.023490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.839917</td>\n",
       "      <td>0.022206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.839917</td>\n",
       "      <td>0.022206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.839917</td>\n",
       "      <td>0.022206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.839917</td>\n",
       "      <td>0.022206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.839917</td>\n",
       "      <td>0.022206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.839917</td>\n",
       "      <td>0.022206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.839917</td>\n",
       "      <td>0.022206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.839917</td>\n",
       "      <td>0.022206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.839917</td>\n",
       "      <td>0.022206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.839917</td>\n",
       "      <td>0.022206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.839917</td>\n",
       "      <td>0.022206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.839917</td>\n",
       "      <td>0.020842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.839897</td>\n",
       "      <td>0.022308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.839897</td>\n",
       "      <td>0.018992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.838538</td>\n",
       "      <td>0.020986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.838518</td>\n",
       "      <td>0.021988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.838518</td>\n",
       "      <td>0.021079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.838518</td>\n",
       "      <td>0.021079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.838518</td>\n",
       "      <td>0.021079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.838518</td>\n",
       "      <td>0.021079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.824512</td>\n",
       "      <td>0.024718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.824482</td>\n",
       "      <td>0.020479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.824473</td>\n",
       "      <td>0.026511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.824473</td>\n",
       "      <td>0.021056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.824473</td>\n",
       "      <td>0.019613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.824463</td>\n",
       "      <td>0.018606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.824463</td>\n",
       "      <td>0.018606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.824463</td>\n",
       "      <td>0.018606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.824463</td>\n",
       "      <td>0.017523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.824453</td>\n",
       "      <td>0.022080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.824453</td>\n",
       "      <td>0.018724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.824453</td>\n",
       "      <td>0.015806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.824443</td>\n",
       "      <td>0.020822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.824443</td>\n",
       "      <td>0.018227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.823104</td>\n",
       "      <td>0.026455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.823094</td>\n",
       "      <td>0.029679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.823074</td>\n",
       "      <td>0.026986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.823074</td>\n",
       "      <td>0.022098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.823074</td>\n",
       "      <td>0.022098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.823064</td>\n",
       "      <td>0.018762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.823054</td>\n",
       "      <td>0.022211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.823054</td>\n",
       "      <td>0.020374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.821676</td>\n",
       "      <td>0.027737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.821676</td>\n",
       "      <td>0.025914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.821636</td>\n",
       "      <td>0.019073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.821636</td>\n",
       "      <td>0.018019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.820277</td>\n",
       "      <td>0.026621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.820267</td>\n",
       "      <td>0.029831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.820237</td>\n",
       "      <td>0.019019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818819</td>\n",
       "      <td>0.017960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>726 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     w1  w2  w3  w4  w5  w7      mean       std\n",
       "654   3   3   1   1   3   3  0.841335  0.022678\n",
       "668   3   3   1   3   2   2  0.841315  0.022767\n",
       "165   1   3   1   1   2   2  0.841315  0.022334\n",
       "417   2   3   1   2   2   3  0.841315  0.022334\n",
       "175   1   3   1   2   2   3  0.841295  0.020139\n",
       "199   1   3   2   2   1   3  0.839936  0.022983\n",
       "367   2   2   2   3   1   1  0.839936  0.022983\n",
       "369   2   2   2   3   1   3  0.839936  0.022983\n",
       "109   1   2   2   1   1   3  0.839936  0.021213\n",
       "416   2   3   1   2   2   2  0.839917  0.023490\n",
       "408   2   3   1   1   2   3  0.839917  0.022206\n",
       "440   2   3   2   2   1   2  0.839917  0.022206\n",
       "441   2   3   2   2   1   3  0.839917  0.022206\n",
       "448   2   3   2   3   1   1  0.839917  0.022206\n",
       "575   3   2   1   2   1   2  0.839917  0.022206\n",
       "601   3   2   2   2   1   1  0.839917  0.022206\n",
       "602   3   2   2   2   1   2  0.839917  0.022206\n",
       "686   3   3   2   2   2   2  0.839917  0.022206\n",
       "687   3   3   2   2   2   3  0.839917  0.022206\n",
       "692   3   3   2   3   1   2  0.839917  0.022206\n",
       "693   3   3   2   3   1   3  0.839917  0.022206\n",
       "201   1   3   2   2   2   2  0.839917  0.020842\n",
       "585   3   2   1   3   1   3  0.839897  0.022308\n",
       "99    1   2   1   3   1   2  0.839897  0.018992\n",
       "680   3   3   2   1   3   2  0.838538  0.020986\n",
       "173   1   3   1   2   2   1  0.838518  0.021988\n",
       "82    1   2   1   1   1   3  0.838518  0.021079\n",
       "90    1   2   1   2   1   2  0.838518  0.021079\n",
       "198   1   3   2   2   1   2  0.838518  0.021079\n",
       "224   1   3   3   2   1   1  0.838518  0.021079\n",
       "..   ..  ..  ..  ..  ..  ..       ...       ...\n",
       "298   2   1   3   1   1   3  0.824512  0.024718\n",
       "56    1   1   3   1   2   1  0.824482  0.020479\n",
       "64    1   1   3   2   1   3  0.824473  0.026511\n",
       "52    1   1   2   3   3   3  0.824473  0.021056\n",
       "67    1   1   3   2   2   3  0.824473  0.019613\n",
       "34    1   1   2   1   3   3  0.824463  0.018606\n",
       "311   2   1   3   2   3   1  0.824463  0.018606\n",
       "562   3   1   3   3   3   1  0.824463  0.018606\n",
       "47    1   1   2   3   2   1  0.824463  0.017523\n",
       "322   2   1   3   3   3   3  0.824453  0.022080\n",
       "316   2   1   3   3   1   3  0.824453  0.018724\n",
       "68    1   1   3   2   3   1  0.824453  0.015806\n",
       "543   3   1   3   1   2   3  0.824443  0.020822\n",
       "75    1   1   3   3   2   2  0.824443  0.018227\n",
       "55    1   1   3   1   1   3  0.823104  0.026455\n",
       "296   2   1   3   1   1   1  0.823094  0.029679\n",
       "62    1   1   3   2   1   1  0.823074  0.026986\n",
       "65    1   1   3   2   2   1  0.823074  0.022098\n",
       "321   2   1   3   3   3   2  0.823074  0.022098\n",
       "320   2   1   3   3   3   1  0.823064  0.018762\n",
       "71    1   1   3   3   1   1  0.823054  0.022211\n",
       "69    1   1   3   2   3   2  0.823054  0.020374\n",
       "297   2   1   3   1   1   2  0.821676  0.027737\n",
       "53    1   1   3   1   1   1  0.821676  0.025914\n",
       "70    1   1   3   2   3   3  0.821636  0.019073\n",
       "74    1   1   3   3   2   1  0.821636  0.018019\n",
       "54    1   1   3   1   1   2  0.820277  0.026621\n",
       "63    1   1   3   2   1   2  0.820267  0.029831\n",
       "78    1   1   3   3   3   2  0.820237  0.019019\n",
       "77    1   1   3   3   3   1  0.818819  0.017960\n",
       "\n",
       "[726 rows x 8 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = LogisticRegression()\n",
    "clf2 = SVC(kernel = 'rbf', C = 1000, gamma = 0.001, probability=True)\n",
    "clf3 = KNeighborsClassifier(n_neighbors = 5)\n",
    "clf4 = RandomForestClassifier(random_state=0, n_estimators=300, bootstrap = False, min_samples_leaf = 7,\n",
    "                                       min_samples_split = 7, max_features = 10, max_depth = None, criterion = 'gini')\n",
    "clf5 = GradientBoostingClassifier(random_state=1, n_estimators=100, loss = 'exponential', learning_rate = 0.1,\n",
    "                                min_samples_leaf = 20, min_samples_split = 3, max_features = 6, max_depth = 16)\n",
    "#clf6 = AdaBoostClassifier(n_estimators=100, algorithm='SAMME', base_estimator = RidgeClassifierCV(), learning_rate = 0.5)\n",
    "clf7 = BaggingClassifier(n_estimators=100, base_estimator = KNeighborsClassifier(n_neighbors = 5), max_features = 6)\n",
    "\n",
    "df = pd.DataFrame(columns=['w1', 'w2', 'w3', 'w4', 'w5', 'w7', 'mean', 'std'])\n",
    "\n",
    "i = 0\n",
    "for w1 in range(1,4):\n",
    "    for w2 in range(1,4):\n",
    "        for w3 in range(1,4):\n",
    "            for w4 in range(1,4):\n",
    "                for w5 in range(1,4):\n",
    "                    for w7 in range(1,4):\n",
    "                        if len(set((w1,w2,w3,w4,w5,w7))) == 1: # skip if all weights are equal\n",
    "                            continue\n",
    "                        eclf = VotingClassifier(estimators=[('lr', clf1), ('svc', clf2), ('knn', clf3), ('rf', clf4), \n",
    "                                                            ('gb', clf5), ('bagg', clf7)], \n",
    "                                                voting='soft', weights=[w1,w2,w3,w4,w5,w7])\n",
    "                        scores = cross_val_score(estimator=eclf, \n",
    "                                                 X=train_data[all_predictors],\n",
    "                                                 y=train_data['Survived'],\n",
    "                                                 cv=5,\n",
    "                                                 scoring='accuracy',\n",
    "                                                  n_jobs=1)\n",
    "\n",
    "                        df.loc[i] = [w1, w2, w3, w4, w5, w7, scores.mean(), scores.std()]\n",
    "                        i += 1\n",
    "\n",
    "df.sort(['mean', 'std'], ascending=[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83798882681564246"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting = VotingClassifier(estimators=[('lr', clf1), ('svc', clf2), ('knn', clf3), ('rf', clf4), ('gb', clf5), \n",
    "                                       ('bagg', clf7)], \n",
    "                                                voting='hard', weights =[3, 3, 1, 1, 3, 3])\n",
    "voting.fit(train_data[all_predictors], train_data['Survived'])\n",
    "voting.score(test_data[all_predictors], test_data['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things that have not been considered here and can potentially improve scores:\n",
    "1) Family size by definition only includes close family, and does not conside aunts, uncles, cousins, grandparents, unmarried couples etc. Extended family size can be recomputed using cabin number.\n",
    "2) Cabin variable was ingnored, because it has a lot of missing values. However, it can be decomposed into deck and cabin number (letter and number) and missing values can be estimated from fare and class. This can be of potential importance, because location of the cabin on board may have high impact on survival.\n",
    "3) Ticket variable was also omitted, becuase I do not see why ticket number would influence survival chances.\n",
    "4) Other essemble classifiers could be consider to improve performance (i.e. Stacking)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
